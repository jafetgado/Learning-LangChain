{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - LLM Fundamentals with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API from .env file\n",
    "api_openai = load_dotenv(dotenv_path='./api.env')\n",
    "api_openai = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10b82efe0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1162da2f0> root_client=<openai.OpenAI object at 0x10b82c100> root_async_client=<openai.AsyncOpenAI object at 0x10b82c070> model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an chat-completion LLM (GPT-3.5-turbo)\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo') # Default params, \n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6p/k8b2fdkn3z36ffyjvjjt_cz4tt91c9/T/ipykernel_48822/4163424536.py:7: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': ', the rain pelting against the window panes in a relentless drumbeat. Thunder rumbled in the distance, the flashes of lightning illuminating the sky in an eerie yet beautiful display. The wind howled through the trees, bending them to its will.\\n\\nIn the midst of the storm, a lone figure trudged through the muddy streets, their coat pulled tightly around them to shield from the chill. Lightning cracked overhead, casting their shadow in sharp relief against the buildings. They quickened their pace, eager to reach their destination before the storm grew fiercer.\\n\\nAs they reached the old, abandoned mansion at the edge of town, a chill ran down their spine. The house loomed ominously in the dark, its windows boarded up and its gardens overgrown. But despite its eerie appearance, the figure pushed open the creaking gate and stepped inside.\\n\\nInside, the air was heavy with the scent of decay and neglect. Dust motes danced in the dim light of their lantern, casting eerie shadows on the walls. They navigated the winding corridors, the floorboards groaning under their weight.\\n\\nFinally, they reached the room they sought, a small study with a lone candle flickering on the desk. They approached the desk, their hands shaking as they picked up the letter that lay there. As they read its contents, their heart sank.\\n\\nThe storm raged on outside, but inside the mansion, a different kind of darkness loomed. And as the figure stood there in the dim light, they knew that they were not alone in the house. Something sinister watched from the shadows, waiting for its chance to strike.',\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 329,\n",
       "   'prompt_tokens': 15,\n",
       "   'total_tokens': 344,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'system_fingerprint': None,\n",
       "  'id': 'chatcmpl-BeYVTckuTMvI3C5xvLhxNUtVfWBNT',\n",
       "  'service_tier': 'default',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run--fbf6bbb2-30a2-4efd-8de6-c2d9986ac069-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 15,\n",
       "  'output_tokens': 329,\n",
       "  'total_tokens': 344,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate with model\n",
    "prompt = \"It was a dark and stormy night\"\n",
    "response = model.invoke(prompt)\n",
    "print(type(response))\n",
    "\n",
    "# Response as dictionary\n",
    "response = response.dict()\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, the rain pelting against the window panes in a relentless drumbeat. Thunder rumbled in the distance, the flashes of lightning illuminating the sky in an eerie yet beautiful display. The wind howled through the trees, bending them to its will.\n",
      "\n",
      "In the midst of the storm, a lone figure trudged through the muddy streets, their coat pulled tightly around them to shield from the chill. Lightning cracked overhead, casting their shadow in sharp relief against the buildings. They quickened their pace, eager to reach their destination before the storm grew fiercer.\n",
      "\n",
      "As they reached the old, abandoned mansion at the edge of town, a chill ran down their spine. The house loomed ominously in the dark, its windows boarded up and its gardens overgrown. But despite its eerie appearance, the figure pushed open the creaking gate and stepped inside.\n",
      "\n",
      "Inside, the air was heavy with the scent of decay and neglect. Dust motes danced in the dim light of their lantern, casting eerie shadows on the walls. They navigated the winding corridors, the floorboards groaning under their weight.\n",
      "\n",
      "Finally, they reached the room they sought, a small study with a lone candle flickering on the desk. They approached the desk, their hands shaking as they picked up the letter that lay there. As they read its contents, their heart sank.\n",
      "\n",
      "The storm raged on outside, but inside the mansion, a different kind of darkness loomed. And as the figure stood there in the dim light, they knew that they were not alone in the house. Something sinister watched from the shadows, waiting for its chance to strike.\n"
     ]
    }
   ],
   "source": [
    "# View generated text\n",
    "text = prompt + response['content']\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat completion with specific roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Messages sent to LLMs are classified into roles, based on the function of the message\n",
    "\n",
    "* System role: instructions the model should use to answer a question\n",
    "\n",
    "* User role: User's query and content provided by the user\n",
    "\n",
    "* Assistant role: content generated by the model\n",
    "\n",
    "* Roles are achieved with chat message interfaces, instead of a simple string as prompt. E.g. HumanMessage for user role, AIMessage for assistant role, SystemMessage for system role, and ChatMessage for arbitrary role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# User role example (HumanMessage)\n",
    "prompt_text = 'What is the capital of France?'\n",
    "prompt = [HumanMessage(prompt_text)]\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris!!!\n"
     ]
    }
   ],
   "source": [
    "# System role example (SystemMessage)\n",
    "system_msg = SystemMessage(\n",
    "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
    ")\n",
    "human_msg = HumanMessage(\"What is the capital of France?\")\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat completions with a standard prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can specify a prompt template and use LangChain's PrompTemplate to construct prompts with dynamic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
      "\n",
      "Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\n",
      "\n",
      "Question: Which model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Specifying a prompt template\n",
    "prompt_template =  \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Dynamic template inputs\n",
    "template = PromptTemplate.from_template(prompt_template)\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we can feed the template-constructed prompt to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face, OpenAI, and Cohere offer LLMs.\n"
     ]
    }
   ],
   "source": [
    "# Feed prompt to model\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat completions with a role-based prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct the prompt template with specified roles (user, system, assistant)\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI and Cohere offer LLMs through their libraries.\n"
     ]
    }
   ],
   "source": [
    "# Pass the prompt to the model to generate a response\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting specific format out of the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Use BaseModel from pydantic to define the schema for the format that you want the LLM to respect\n",
    "\n",
    "* Then you include the schema in the prompt.\n",
    "\n",
    "* Pydantic is a Python library used for data validation and settings management using Python type annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.AnswerWithJustification'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'They weigh the same',\n",
       " 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Schema for JSON format\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "# Initialize a model with the defined schema\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "# Generate the response\n",
    "response = structured_llm.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\")\n",
    "print(type(response)) # A pydantic object\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'They weigh the same',\n",
       " 'justification': 'A pound is a unit of weight, so a pound of bricks and a pound of feathers both weigh the same amount, which is one pound.',\n",
       " 'alternative_explanation': 'Although bricks are denser and heavier than feathers, a pound of bricks and a pound of feathers still weigh the same because they are both measured in pounds.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand the schema with alternative explanation\n",
    "\n",
    "class AnswerWithJustificationAndExplanation(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "    alternative_explanation: str\n",
    "    \"\"\"An alternative explanation for the answer\"\"\"\n",
    "\n",
    "\n",
    "# Initialize a model with the defined schema\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustificationAndExplanation)\n",
    "\n",
    "# Generate the response\n",
    "response = structured_llm.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the runnable interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* .invoke -> transforms single input to output\n",
    "\n",
    "* .batch -> efficiently transforms multiple inputs to multiple outputs\n",
    "\n",
    "* .stream -> streams ouptut from ingle input as it is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Invoke example\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "completion = model.invoke(\"Hi there!\")\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Hello! How can I assist you today?', 'Goodbye! Have a great day!']\n"
     ]
    }
   ],
   "source": [
    "# Batch example\n",
    "completions = model.batch([\"Hi there!\", \"Bye!\"])\n",
    "print(type(completions)) # list\n",
    "print([item.content for item in completions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Good\n",
      "bye\n",
      "!\n",
      " Have\n",
      " a\n",
      " great\n",
      " day\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream example\n",
    "# Returns a generator that yields tokens or chunks of output one at a time as the model generates them\n",
    "for token in model.stream(\"Bye!\"):\n",
    "    print(token.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imperative and declarative composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two ways of composing model calls\n",
    "    \n",
    "    1. Imperative composition -> you build explicit code into functions/classes and use them as langchain runnables\n",
    "    \n",
    "    2. Declarative composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some popular model providers that offer LLMs (large language models) are:\n",
      "\n",
      "1. OpenAI (GPT-3)\n",
      "2. Google (BERT, T5)\n",
      "3. Facebook AI Research (RoBERTa)\n",
      "4. Hugging Face (Transformer, BERT, GPT-2)\n",
      "5. Microsoft (Turing-NLG, MT-DNN)\n",
      "6. Salesforce (CTRL)\n",
      "7. EleutherAI (GPT-Neo) \n",
      "\n",
      "These providers offer a range of LLMs with different capabilities and use cases.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imperative composition example\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    # Combine the prompt and model into a function\n",
    "    # The @chain decorator adds the same Runnable interface for any function you write\n",
    "    # @chain wraps your function, so that it behaves like a langchain runnable with .invoke, .batch, )\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# Run the function\n",
    "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs (language models)?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above approach only works for .invoke and .batch\n",
    "\n",
    "* To use .stream, modify the code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The\n",
      " three\n",
      " most\n",
      " popular\n",
      " providers\n",
      " of\n",
      " L\n",
      "LM\n",
      "s\n",
      " (\n",
      "language\n",
      " models\n",
      ")\n",
      " are\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " Open\n",
      "AI\n",
      "'s\n",
      " G\n",
      "PT\n",
      " (\n",
      "Gener\n",
      "ative\n",
      " Pre\n",
      "-trained\n",
      " Transformer\n",
      ")\n",
      "\n",
      "2\n",
      ".\n",
      " Google\n",
      "'s\n",
      " B\n",
      "ERT\n",
      " (\n",
      "Bid\n",
      "irectional\n",
      " Encoder\n",
      " Represent\n",
      "ations\n",
      " from\n",
      " Transformers\n",
      ")\n",
      "\n",
      "3\n",
      ".\n",
      " Facebook\n",
      "'s\n",
      " Ro\n",
      "BERT\n",
      "a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imperative composition with stream\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "\n",
    "for part in chatbot.stream({\"question\": \"List the three most popular providers of LLMs (language models)?\"}):\n",
    "    print(part.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For asynchronous execution, use the python keywords: async and await \n",
    "\n",
    "* Asynchronous  means that the generation can be allowed to run in the background without blocking the main flow\n",
    "\n",
    "* Async. processes are efficient for high-latency tasks.\n",
    "\n",
    "* With async. execution, you are telling Python: \"Start this I/O-bound operation (the API call to the LLM), and while you're waiting for the response, you’re allowed to handle other tasks (other coroutines).\" The LLM server still generates one token at a time.\n",
    "\n",
    "* The async call just allows your Python process to do other things while the LLM is busy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Asynchronous imperative composition\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    return await chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    print(asyncio.run(main()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarative composition\n",
    "\n",
    "* With declarative composition, you use LCEL (LangChain Experession Language)\n",
    "\n",
    "* LCEL is a declarative, chainable syntax that enables you compose complex pipelines of components in a structured and modular way (e.g. similar to | in bash pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several companies offer language models as part of their services. Some popular options for language models include:\n",
      "\n",
      "1. OpenAI, which offers the GPT series of language models, including GPT-2 and GPT-3.\n",
      "2. Google Cloud's Natural Language API provides access to a powerful pretrained language model.\n",
      "3. Microsoft Azure has language models available through its Text Analytics API.\n",
      "4. IBM Watson offers language models as part of its Natural Language Understanding service.\n",
      "5. Amazon Web Services (AWS) provides language models through its Amazon Comprehend service.\n",
      "\n",
      "These are just a few options, and there are many other companies and research organizations that also offer language models for various purposes.\n"
     ]
    }
   ],
   "source": [
    "# Declarative composition\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "# # Combine model and template with the LCEL pipeline operator ( | ) \n",
    "chatbot = template | model\n",
    "\n",
    "# Execute the generation with invoke/batch\n",
    "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs (language models)?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute with streaming\n",
    "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs (language models)?\"}):\n",
    "    print(part.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
